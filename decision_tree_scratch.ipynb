{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f21e39c8-5065-4322-9c41-214bda8f3885",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "In this document, we implement the decision tree algorithm (machine learning model) from scratch in Python. To this end, we use the guidelines provided by AssemblyAI in https://www.youtube.com/watch?v=NxEHSAfFlK8.\n",
    "\n",
    "It is worth mentioning that, we use Information Gain (IG) as the metric to split data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c050bc4-28a1-489f-b4b6-a716cdd2510a",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "552a7bb5-ccdb-4085-b297-48387011d7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0e8578-42b0-44c4-8637-8d683463c493",
   "metadata": {},
   "source": [
    "## Classes\n",
    "For a tree, two classes are required. One is the <em>Node</em> class to create the nodes in the tree, and the other is to form the tree itself based on\n",
    "the features and the splitting criteria (here is IG)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52057bc2-671f-4a47-b1fd-3a34277488be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, *, value=None):\n",
    "        self.feature   = feature     # feature to split\n",
    "        self.threshold = threshold   # threshold to split\n",
    "        self.left      = left        # left child\n",
    "        self.right     = right       # right child\n",
    "        self.value     = value       # value\n",
    "\n",
    "    def is_leaf_node(self):\n",
    "        # return self.value is not None\n",
    "        return self.left is None and self.right is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82efe72e-f0e8-48cb-955b-446d2d07b81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, min_samples_split=2, max_depth=100, n_features=None,\n",
    "                 mode='classification'):\n",
    "        self.min_samples_split = min_samples_split     # stopping criterion: minimum number of samples in a leaf node\n",
    "        self.max_depth         = max_depth             # stopping criterion: maximum depth of the tree\n",
    "        self.n_features        = n_features            # number of features\n",
    "        self.root              = None\n",
    "        self.mode              = mode.lower()          # classification or regression\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.n_features = X.shape[1] if not self.n_features else min(self.n_features, X.shape[1])\n",
    "        self.root = self._build_tree(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
    "\n",
    "    ################################### Auxiliary Functions #####################################\n",
    "    # build the tree\n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        n_samples, n_features = X.shape\n",
    "        n_labels              = len(np.unique(y))\n",
    "\n",
    "        # check stopping criteria: min_samples_split, max_depth, n_labels            \n",
    "        if (n_samples < self.min_samples_split or depth >= self.max_depth or n_labels == 1):\n",
    "            if self.mode == 'classification':\n",
    "                counter = Counter(y)\n",
    "                leaf_value = counter.most_common(1)[0][0]\n",
    "            elif self.mode == 'regression':\n",
    "                leaf_value = np.mean(y)\n",
    "            return Node(value=leaf_value)\n",
    "\n",
    "        # find the best split\n",
    "                # randomly selects a set of features (self.n_features) from n_features\n",
    "        feature_indices = np.random.choice(n_features, self.n_features, replace=False)\n",
    "        best_feature, best_threshold = self._find_best_split(X, y, feature_indices)\n",
    "\n",
    "        # create child nodes\n",
    "        left_index, right_index = self._split(X[:, best_feature], best_threshold)\n",
    "        left = self._build_tree(X[left_index, :], y[left_index], depth + 1)\n",
    "        right = self._build_tree(X[right_index, :], y[right_index], depth + 1)\n",
    "\n",
    "        return Node(best_feature, best_threshold, left, right)\n",
    "\n",
    "\n",
    "    # find the best feature and threshold for splitting\n",
    "    def _find_best_split(self, X, y, feature_indices):\n",
    "        best_gain = -1\n",
    "        split_index, split_threshold = None, None\n",
    "        for feature_index in feature_indices:\n",
    "            X_col = X[:, feature_index]\n",
    "            thresholds = np.unique(X_col)\n",
    "            for threshold in thresholds:\n",
    "                # calculate IG\n",
    "                gain = self._gain(X_col, y, threshold)\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    split_index = feature_index\n",
    "                    split_threshold = threshold\n",
    "\n",
    "        return split_index, split_threshold\n",
    "\n",
    "    # determine gain based on the mode\n",
    "    def _gain(self, X_col, y, threshold):\n",
    "        if self.mode == \"classification\":\n",
    "            return self._information_gain(X_col, y, threshold)\n",
    "        elif self.mode == \"regression\":\n",
    "            return self._variance_reduction(X_col, y, threshold)\n",
    "\n",
    "    # calculate information gain (for classification)\n",
    "    def _information_gain(self, X, y, threshold):\n",
    "        # parent entropy: entropy before splitting (w.r.t. labels)\n",
    "        H_parent = self._entropy(y)\n",
    "\n",
    "        # children entropy: entropy after splitting based on the threshold\n",
    "        # create children\n",
    "        left_indices, right_indices = self._split(X, threshold)\n",
    "        n_left, n_right             = len(left_indices), len(right_indices)\n",
    "        \n",
    "        if n_left == 0 or n_right == 0:\n",
    "            return 0              # IG = 0\n",
    "\n",
    "        # calculate the weighted average entropy of children\n",
    "        n = len(y)\n",
    "        H_left, H_right = self._entropy(y[left_indices]), self._entropy(y[right_indices])\n",
    "        H_children = (n_left/n) * H_left + (n_right/n) * H_right\n",
    "\n",
    "        # calculate the IG\n",
    "        IG = H_parent - H_children\n",
    "        return IG\n",
    "\n",
    "    # calculate variance reduction (for regression)\n",
    "    def _variance_reduction(self, X, y, threshold):\n",
    "        left_indices, right_indices = self._split(X, threshold)\n",
    "        n_left, n_right             = len(left_indices), len(right_indices)\n",
    "        \n",
    "        if n_left == 0 or n_right == 0:\n",
    "            return 0              # variance = 0\n",
    "    \n",
    "        n = len(y)\n",
    "        var_total = np.var(y)\n",
    "        var_left, var_right = np.var(y[left_indices]), np.var(y[right_indices])\n",
    "    \n",
    "        weighted_var = (n_left / n) * var_left + (n_right / n) * var_right\n",
    "        reduction = var_total - weighted_var\n",
    "        return reduction\n",
    "\n",
    "    # form children\n",
    "    def _split(self, X, threshold):\n",
    "        left_index  = np.argwhere(X <= threshold).flatten()\n",
    "        right_index = np.argwhere(X  > threshold).flatten()\n",
    "        return left_index, right_index\n",
    "\n",
    "    # calculate entropy\n",
    "    def _entropy(self, y):\n",
    "        hist = np.bincount(y)\n",
    "        probabilities = hist / len(y)\n",
    "        return -np.sum([p * np.log(p) for p in probabilities if p > 0])\n",
    "\n",
    "    # traverse tree for predictions\n",
    "    def _traverse_tree(self, X, node):\n",
    "        if node.is_leaf_node():\n",
    "            return node.value\n",
    "\n",
    "        if X[node.feature] <= node.threshold:\n",
    "            return self._traverse_tree(X, node.left)\n",
    "\n",
    "        return self._traverse_tree(X, node.right)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fc60c8-cba9-4f62-904d-e76a41f6bd65",
   "metadata": {},
   "source": [
    "## Fit and Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74b3fa74-4d20-4c8e-a91b-8cbc1c01a8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7221f5-b8af-4821-b714-9fc8fb51ce6e",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a18fefed-d810-490e-9364-5db99ea8b684",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification():\n",
    "    dataset = datasets.load_breast_cancer()\n",
    "    X, y = dataset.data, dataset.target\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    clf = DecisionTree()\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = np.sum(y_pred == y_test) / len(y_test)\n",
    "    print(f\"Results for Classification - Accuracy: {accuracy*100:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a72df30-9464-49ab-b35a-2ccc97f8e13c",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0068ba91-74cb-4a4e-867d-388028566182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression():\n",
    "    dataset = datasets.fetch_california_housing()\n",
    "    X, y = dataset.data, dataset.target\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    reg = DecisionTree(mode='regression')\n",
    "    reg.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = reg.predict(X_test)\n",
    "    loss = mean_squared_error(y_pred, y_test)\n",
    "    print(f\"Results for Regression - Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bd026ce-056a-4c4a-9c81-8196028662a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Classification - Accuracy: 94.74\n",
      "Results for Regression - Loss: 0.5070\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    classification()\n",
    "    regression()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
