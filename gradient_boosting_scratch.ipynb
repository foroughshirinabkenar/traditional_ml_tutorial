{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2deda2d1-60ee-42db-bdf3-fc729f09f9c5",
   "metadata": {},
   "source": [
    "# Gradient Boosting\n",
    "In this document, we implement the gradient boosting algorithm (machine learning model) from scratch in Python.\n",
    "\n",
    "It is worth mentioning that, since gradient boositng comprises multiple sequential trees (decision trees), we import the DecisionTree class prvided in \"decision_tree_scratch.ipynb\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056d1b0b-a3fd-4fc5-8052-006346f527f6",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e465df11-184e-4b4a-b690-9c80468756f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import import_ipynb\n",
    "from decision_tree_scratch import DecisionTree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbc7fe6-cdb0-45ff-a10a-20c93d9e4cd1",
   "metadata": {},
   "source": [
    "## Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f363bd8-5319-4f9f-9f9d-dd9693cbcc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientBoosting:\n",
    "    def __init__(self, n_trees=10, min_samples_split=2, max_depth=10, n_features=None,\n",
    "                learning_rate=0.1, mode='classification'):\n",
    "        self.n_trees           = n_trees               # number of trees in the forest\n",
    "        self.min_samples_split = min_samples_split     # stopping criterion: minimum number of samples in a leaf node\n",
    "        self.max_depth         = max_depth             # stopping criterion: maximum depth of the tree\n",
    "        self.n_features        = n_features            # number of features\n",
    "        self.trees             = []                    # list of trees for the forest\n",
    "        self.learning_rate     = learning_rate         # learning rate\n",
    "        self.initial_pred      = None                  # initail preds based on the prior probability of the positive class\n",
    "        self.mode              = mode.lower()          # classification or regression\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.trees = []\n",
    "        for _ in range(self.n_trees):\n",
    "            tree = DecisionTree(self.min_samples_split, self.max_depth, self.n_features, mode=self.mode)\n",
    "            X_sample, y_sample = self._bootstrap_samples(X, y)\n",
    "            tree.fit(X, y)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "        if self.mode == 'classification':\n",
    "            # log-odds for initialization\n",
    "            p = np.clip(np.mean(y), 1e-10, 1 - 1e-10)  # to avoid divide-by-zero\n",
    "            self.initial_pred = np.log(p / (1 - p))\n",
    "        elif self.mode == 'regression':\n",
    "            self.initial_pred = np.mean(y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # fill with the initial log-odds\n",
    "        preds = np.full(X.shape[0], self.initial_pred)\n",
    "\n",
    "        # apply trees\n",
    "        for tree in self.trees:\n",
    "            preds += self.learning_rate * tree.predict(X)\n",
    "\n",
    "        if self.mode == 'regression':\n",
    "            return preds\n",
    "\n",
    "        # convert values (log-odds), called preds here, to probabilities using sigmoid function\n",
    "        probs = 1 / (1 + np.exp(-preds))\n",
    "        \n",
    "        predictions = np.where(probs >= 0.5, 1, 0)\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "    ################################### Auxiliary Functions #####################################\n",
    "    # form bootstrap (smapling) data\n",
    "    def _bootstrap_samples(self, X, y):\n",
    "        n_samples = X.shape[0]\n",
    "        indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "        \n",
    "        return X[indices], y[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338f43e8-8d5a-4ebe-a438-da1fe76cb0c4",
   "metadata": {},
   "source": [
    "## Fit and Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88b6425e-4de4-4a7c-a570-9fe326f096e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e115633-d920-48bf-b71a-6ae5cd50009e",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2db94d61-3a87-4bef-8b47-eeb00c32b1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification():\n",
    "    dataset = datasets.load_breast_cancer()\n",
    "    X, y = dataset.data, dataset.target\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    clf = GradientBoosting()\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = np.sum(y_pred == y_test) / len(y_test)\n",
    "    print(f\"Results for Classification - Accuracy: {accuracy*100:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732a126b-d032-4a5e-a6d2-ffdaf321a1bd",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf535b6b-5680-4f6f-ba4e-d25ffdeace1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression():\n",
    "    dataset = datasets.fetch_california_housing()\n",
    "    X, y = dataset.data, dataset.target\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    reg = GradientBoosting(mode='regression')\n",
    "    reg.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = reg.predict(X_test)\n",
    "    loss = mean_squared_error(y_pred, y_test)\n",
    "    print(f\"Results for Regression - Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d95c9c2b-859f-45c8-9b21-770886ffd2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Classification - Accuracy: 62.28\n",
      "Results for Regression - Loss: 4.7000\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    classification()\n",
    "    regression()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
